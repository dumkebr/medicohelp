import React, { useEffect, useRef, useState } from "react";

// ────────────────────────────────────────────────────────────────
// MÉDICOHELP — BARRA DE ANEXOS, FOTO E ÁUDIO (COM DITADO)
// Comandos:
// - Upload de arquivos (imagem, PDF, áudio, vídeo, etc.)
// - Tirar foto da câmera (mobile/desktop com câmera)
// - Gravar áudio (para enviar) e Ditado por voz → texto (Web Speech API)
// - Drag & Drop
// Estilo: Tailwind (shadcn/ui opcional). Ícones simples em SVG inline.
// Callbacks: onFilesSelected(File[]), onTextFromSpeech(string)
// 
// IMPORTANTE:
// 1) O "capture" do <input> funciona melhor em Android/Chrome. iOS varia.
// 2) O Ditado usa Web Speech API (Chrome/Edge). Em outros, usa fallback de gravação.
// 3) Para transcrever gravações no backend, crie um endpoint /api/transcribe.
// 4) Para armazenar arquivos, crie /api/upload (exemplos no final deste arquivo).
// ────────────────────────────────────────────────────────────────

export type AttachmentBarProps = {
  onFilesSelected?: (files: File[]) => void;
  onTextFromSpeech?: (text: string) => void;
  // Use estas URLs se já tiver seus endpoints prontos
  uploadUrl?: string; // ex.: "/api/upload"
  transcribeUrl?: string; // ex.: "/api/transcribe"
  // Mostrar preview de imagens selecionadas
  showPreviews?: boolean;
};

const AttachmentBar: React.FC<AttachmentBarProps> = ({
  onFilesSelected,
  onTextFromSpeech,
  uploadUrl,
  transcribeUrl,
  showPreviews = true,
}) => {
  const fileInputRef = useRef<HTMLInputElement | null>(null);
  const cameraInputRef = useRef<HTMLInputElement | null>(null);

  // Câmera ao vivo
  const videoRef = useRef<HTMLVideoElement | null>(null);
  const streamRef = useRef<MediaStream | null>(null);
  const [cameraOn, setCameraOn] = useState(false);
  const [snapshots, setSnapshots] = useState<string[]>([]);

  // Áudio: Ditado (Web Speech API)
  const [isDictating, setIsDictating] = useState(false);
  const recognitionRef = useRef<any>(null);
  const [dictationText, setDictationText] = useState("");

  // Áudio: Gravação (MediaRecorder)
  const [isRecording, setIsRecording] = useState(false);
  const mediaRecorderRef = useRef<MediaRecorder | null>(null);
  const audioChunksRef = useRef<Blob[]>([]);

  const [droppedFiles, setDroppedFiles] = useState<File[]>([]);
  const [previews, setPreviews] = useState<string[]>([]);

  // ─────────────────────────────
  // Helpers de Upload
  async function uploadFiles(files: File[]) {
    if (!files?.length) return;
    onFilesSelected?.(files);

    // Upload opcional para o seu backend
    if (uploadUrl) {
      const form = new FormData();
      files.forEach((f) => form.append("files", f));
      try {
        await fetch(uploadUrl, { method: "POST", body: form });
      } catch (e) {
        console.error("Falha no upload", e);
      }
    }

    if (showPreviews) {
      const urls = files
        .filter((f) => f.type.startsWith("image/"))
        .map((f) => URL.createObjectURL(f));
      setPreviews((prev) => [...urls, ...prev]);
    }
  }

  // ─────────────────────────────
  // Botão: Arquivos
  function handlePickFiles() {
    fileInputRef.current?.click();
  }

  function handleFilesChange(e: React.ChangeEvent<HTMLInputElement>) {
    const files = e.target.files ? Array.from(e.target.files) : [];
    if (files.length) {
      uploadFiles(files);
      // Limpa para permitir re-selecionar o mesmo arquivo
      e.target.value = "";
    }
  }

  // ─────────────────────────────
  // Botão: Câmera (nativo via capture)
  function handlePickCamera() {
    cameraInputRef.current?.click();
  }

  function handleCameraFile(e: React.ChangeEvent<HTMLInputElement>) {
    const files = e.target.files ? Array.from(e.target.files) : [];
    if (files.length) {
      uploadFiles(files);
      e.target.value = "";
    }
  }

  // ─────────────────────────────
  // Câmera ao vivo (getUserMedia)
  async function toggleLiveCamera() {
    if (cameraOn) {
      stopCamera();
      setCameraOn(false);
      return;
    }
    try {
      const stream = await navigator.mediaDevices.getUserMedia({ video: true });
      streamRef.current = stream;
      if (videoRef.current) {
        videoRef.current.srcObject = stream;
        await videoRef.current.play();
      }
      setCameraOn(true);
    } catch (err) {
      console.error("Erro ao acessar câmera:", err);
      alert("Não consegui acessar a câmera. Verifique permissões.");
    }
  }

  function stopCamera() {
    streamRef.current?.getTracks().forEach((t) => t.stop());
    streamRef.current = null;
  }

  function takeSnapshot() {
    if (!videoRef.current) return;
    const video = videoRef.current;
    const canvas = document.createElement("canvas");
    canvas.width = video.videoWidth || 1280;
    canvas.height = video.videoHeight || 720;
    const ctx = canvas.getContext("2d");
    if (!ctx) return;
    ctx.drawImage(video, 0, 0, canvas.width, canvas.height);
    const dataUrl = canvas.toDataURL("image/png");
    setSnapshots((prev) => [dataUrl, ...prev]);

    // Converte em File para upload
    canvas.toBlob((blob) => {
      if (!blob) return;
      const file = new File([blob], `foto_${Date.now()}.png`, { type: "image/png" });
      uploadFiles([file]);
    }, "image/png");
  }

  useEffect(() => {
    return () => {
      stopCamera();
      previews.forEach((u) => URL.revokeObjectURL(u));
    };
    // eslint-disable-next-line react-hooks/exhaustive-deps
  }, []);

  // ─────────────────────────────
  // Ditado (Web Speech API)
  function hasSpeechApi() {
    return (
      typeof window !== "undefined" &&
      ((window as any).webkitSpeechRecognition || (window as any).SpeechRecognition)
    );
  }

  function startDictation() {
    if (!hasSpeechApi()) {
      alert(
        "Seu navegador não suporta ditado em tempo real. Use Chrome/Edge ou use a gravação de áudio."
      );
      return;
    }
    const SpeechRecognition =
      (window as any).webkitSpeechRecognition || (window as any).SpeechRecognition;
    const recognition = new SpeechRecognition();
    recognition.lang = "pt-BR";
    recognition.continuous = true;
    recognition.interimResults = true;

    recognition.onresult = (event: any) => {
      let interim = "";
      let finalText = "";
      for (let i = event.resultIndex; i < event.results.length; i++) {
        const res = event.results[i];
        if (res.isFinal) finalText += res[0].transcript + " ";
        else interim += res[0].transcript;
      }
      const text = (dictationText + " " + finalText + " " + interim).trim();
      setDictationText(text);
      onTextFromSpeech?.(text);
    };

    recognition.onend = () => {
      setIsDictating(false);
    };

    recognition.onerror = (e: any) => {
      console.error(e);
      setIsDictating(false);
    };

    recognition.start();
    recognitionRef.current = recognition;
    setIsDictating(true);
  }

  function stopDictation() {
    try {
      recognitionRef.current?.stop();
    } catch {}
    setIsDictating(false);
  }

  // ─────────────────────────────
  // Gravar Áudio (MediaRecorder) → upload e/ou transcrição no backend
  async function startRecording() {
    try {
      const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
      const rec = new MediaRecorder(stream);
      audioChunksRef.current = [];
      rec.ondataavailable = (e) => {
        if (e.data.size > 0) audioChunksRef.current.push(e.data);
      };
      rec.onstop = async () => {
        const blob = new Blob(audioChunksRef.current, { type: "audio/webm" });
        const file = new File([blob], `audio_${Date.now()}.webm`, { type: "audio/webm" });
        await uploadFiles([file]);
        if (transcribeUrl) {
          const form = new FormData();
          form.append("file", file);
          try {
            const r = await fetch(transcribeUrl, { method: "POST", body: form });
            const data = await r.json();
            if (data?.text) {
              onTextFromSpeech?.(data.text);
              setDictationText((prev) => (prev ? prev + " " + data.text : data.text));
            }
          } catch (e) {
            console.error("Falha na transcrição", e);
          }
        }
        // encerra tracks
        stream.getTracks().forEach((t) => t.stop());
      };
      rec.start();
      mediaRecorderRef.current = rec;
      setIsRecording(true);
    } catch (e) {
      console.error("Erro ao iniciar gravação", e);
      alert("Não consegui acessar o microfone. Verifique permissões.");
    }
  }

  function stopRecording() {
    try {
      mediaRecorderRef.current?.stop();
    } catch {}
    setIsRecording(false);
  }

  // ─────────────────────────────
  // Drag & Drop
  function onDrop(e: React.DragEvent<HTMLDivElement>) {
    e.preventDefault();
    const items = e.dataTransfer.items;
    const files: File[] = [];
    if (items) {
      for (let i = 0; i < items.length; i++) {
        const it = items[i];
        if (it.kind === "file") {
          const f = it.getAsFile();
          if (f) files.push(f);
        }
      }
    } else if (e.dataTransfer.files?.length) {
      for (const f of Array.from(e.dataTransfer.files)) files.push(f);
    }
    if (files.length) {
      setDroppedFiles(files);
      uploadFiles(files);
    }
  }

  function onDragOver(e: React.DragEvent<HTMLDivElement>) {
    e.preventDefault();
  }

  // ─────────────────────────────
  // UI
  return (
    <div className="w-full">
      {/* Barra de ações */}
      <div className="flex flex-wrap items-center gap-2 rounded-2xl border p-2 shadow-sm bg-white">
        {/* Upload de arquivos */}
        <button
          onClick={handlePickFiles}
          className="px-3 py-2 rounded-xl border hover:bg-gray-50 active:scale-[.98] transition"
          title="Anexar arquivos"
        >
          <span className="inline-flex items-center gap-2">
            <svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" strokeWidth="2" strokeLinecap="round" strokeLinejoin="round"><path d="M21.44 11.05l-9.19 9.19a6 6 0 01-8.49-8.49l9.19-9.19a4 4 0 015.66 5.66l-9.2 9.19a2 2 0 11-2.83-2.83l8.49-8.48"/></svg>
            Arquivo
          </span>
        </button>
        <input
          ref={fileInputRef}
          type="file"
          multiple
          accept="image/*,application/pdf,audio/*,video/*,application/msword,application/vnd.openxmlformats-officedocument.wordprocessingml.document"
          className="hidden"
          onChange={handleFilesChange}
        />

        {/* Câmera nativa (abre câmera no mobile) */}
        <button
          onClick={handlePickCamera}
          className="px-3 py-2 rounded-xl border hover:bg-gray-50 active:scale-[.98] transition"
          title="Tirar foto (câmera do celular)"
        >
          <span className="inline-flex items-center gap-2">
            <svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" strokeWidth="2" strokeLinecap="round" strokeLinejoin="round"><path d="M23 19a2 2 0 0 1-2 2H3a2 2 0 0 1-2-2V8a2 2 0 0 1 2-2h3l2-3h8l2 3h3a2 2 0 0 1 2 2z"/><circle cx="12" cy="13" r="4"/></svg>
            Foto
          </span>
        </button>
        <input
          ref={cameraInputRef}
          type="file"
          accept="image/*"
          capture="environment"
          className="hidden"
          onChange={handleCameraFile}
        />

        {/* Câmera ao vivo (desktop/mobile) */}
        <button
          onClick={toggleLiveCamera}
          className={`px-3 py-2 rounded-xl border hover:bg-gray-50 active:scale-[.98] transition ${cameraOn ? "bg-gray-100" : ""}`}
          title="Câmera ao vivo"
        >
          <span className="inline-flex items-center gap-2">
            <svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" strokeWidth="2" strokeLinecap="round" strokeLinejoin="round"><rect x="2" y="5" width="15" height="14" rx="2" ry="2"/><polygon points="17 7 22 5 22 19 17 17"/></svg>
            {cameraOn ? "Fechar câmera" : "Câmera ao vivo"}
          </span>
        </button>
        {cameraOn && (
          <button
            onClick={takeSnapshot}
            className="px-3 py-2 rounded-xl border hover:bg-gray-50 active:scale-[.98] transition"
            title="Capturar foto"
          >
            Capturar
          </button>
        )}

        {/* Ditado por voz (tempo real) */}
        {!isDictating ? (
          <button
            onClick={startDictation}
            className="px-3 py-2 rounded-xl border hover:bg-gray-50 active:scale-[.98] transition"
            title="Ditado por voz (tempo real)"
          >
            <span className="inline-flex items-center gap-2">
              <svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" strokeWidth="2" strokeLinecap="round" strokeLinejoin="round"><path d="M12 1a3 3 0 00-3 3v7a3 3 0 006 0V4a3 3 0 00-3-3z"/><path d="M19 10v2a7 7 0 01-14 0v-2"/><line x1="12" y1="19" x2="12" y2="23"/><line x1="8" y1="23" x2="16" y2="23"/></svg>
              Falar (ditado)
            </span>
          </button>
        ) : (
          <button
            onClick={stopDictation}
            className="px-3 py-2 rounded-xl border bg-red-50 hover:bg-red-100 active:scale-[.98] transition"
            title="Parar ditado"
          >
            Parar ditado
          </button>
        )}

        {/* Gravar áudio (arquivo) */}
        {!isRecording ? (
          <button
            onClick={startRecording}
            className="px-3 py-2 rounded-xl border hover:bg-gray-50 active:scale-[.98] transition"
            title="Gravar áudio e enviar"
          >
            <span className="inline-flex items-center gap-2">
              <svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" strokeWidth="2" strokeLinecap="round" strokeLinejoin="round"><circle cx="12" cy="12" r="3"/><rect x="2" y="4" width="20" height="16" rx="2"/></svg>
              Gravar áudio
            </span>
          </button>
        ) : (
          <button
            onClick={stopRecording}
            className="px-3 py-2 rounded-xl border bg-red-50 hover:bg-red-100 active:scale-[.98] transition"
            title="Parar gravação"
          >
            Parar gravação
          </button>
        )}

        {/* Área de Drop */}
        <div
          onDrop={onDrop}
          onDragOver={onDragOver}
          className="ml-auto rounded-xl border border-dashed px-3 py-2 text-sm text-gray-600"
          title="Arraste e solte arquivos aqui"
        >
          Arraste & solte aqui
        </div>
      </div>

      {/* Pré-visualização / Câmera ao vivo */}
      <div className="mt-3 grid grid-cols-1 md:grid-cols-2 gap-3">
        {cameraOn && (
          <div className="rounded-2xl overflow-hidden border">
            <video ref={videoRef} className="w-full h-auto bg-black" playsInline muted />
          </div>
        )}

        {showPreviews && (previews.length > 0 || snapshots.length > 0) && (
          <div className="rounded-2xl border p-2">
            <div className="text-sm font-medium mb-2">Pré-visualizações</div>
            <div className="grid grid-cols-3 gap-2">
              {snapshots.map((src, idx) => (
                <img key={`snap-${idx}`} src={src} alt="snapshot" className="rounded-lg border" />
              ))}
              {previews.map((src, idx) => (
                <img key={`prev-${idx}`} src={src} alt="preview" className="rounded-lg border" />
              ))}
            </div>
          </div>
        )}
      </div>

      {/* Texto do ditado (live) */}
      {dictationText && (
        <div className="mt-3 rounded-2xl border p-3 text-sm bg-gray-50">
          <div className="font-semibold mb-1">Ditado (texto capturado)</div>
          <div className="whitespace-pre-wrap">{dictationText}</div>
        </div>
      )}
    </div>
  );
};

export default AttachmentBar;

// ────────────────────────────────────────────────────────────────
// Como usar:
// <AttachmentBar
//   onFilesSelected={(files) => console.log(files)}
//   onTextFromSpeech={(text) => setMensagem(text)}
//   uploadUrl="/api/upload"
//   transcribeUrl="/api/transcribe"
// />
// ────────────────────────────────────────────────────────────────

/*
─────────────────────────────────────────────────────────────────
BACKEND (EXEMPLOS)

1) NEXT.JS (app router) — /app/api/upload/route.ts

import { NextRequest, NextResponse } from "next/server";
import { writeFile } from "fs/promises";

export async function POST(req: NextRequest) {
  const formData = await req.formData();
  const files = formData.getAll("files") as File[];
  for (const f of files) {
    const bytes = Buffer.from(await f.arrayBuffer());
    await writeFile(`/tmp/${Date.now()}_${f.name}`, bytes);
  }
  return NextResponse.json({ ok: true });
}

2) NEXT.JS — /app/api/transcribe/route.ts (placeholder)
// Envie o arquivo para seu serviço de transcrição (ex.: Whisper API) e retorne {text}

import { NextRequest, NextResponse } from "next/server";

export async function POST(req: NextRequest) {
  const formData = await req.formData();
  const file = formData.get("file") as File;
  // TODO: chamar seu provedor de ASR aqui
  // const text = await transcrever(file);
  const text = "(transcrição de exemplo)";
  return NextResponse.json({ text });
}

3) EXPRESS + MULTER (Node)

import express from "express";
import multer from "multer";
const app = express();
const upload = multer({ dest: "uploads/" });

app.post("/api/upload", upload.array("files"), (req, res) => {
  // req.files contém os anexos
  return res.json({ ok: true });
});

app.post("/api/transcribe", upload.single("file"), async (req, res) => {
  // Envie req.file para o seu serviço de transcrição e retorne {text}
  return res.json({ text: "(transcrição de exemplo)" });
});

app.listen(3000);

─────────────────────────────────────────────────────────────────
NOTAS DE COMPATIBILIDADE
- Web Speech API (ditado) funciona melhor no Chrome/Edge sob HTTPS.
- Em iOS, use principalmente o input capture="environment" para abrir a câmera do iPhone no Safari.
- MediaRecorder de áudio tem suporte amplo em navegadores modernos.
- Sempre publicar sob HTTPS (Vercel já entrega). Permissões de câmera/microfone exigem contexto seguro.
─────────────────────────────────────────────────────────────────
*/
