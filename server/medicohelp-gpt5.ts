/**
 * M√©dicoHelp GPT-5 Integration
 * Sistema h√≠brido com prompts m√©dicos refinados e streaming
 * ATUALIZADO: Usa nova API client.responses.create() (GPT-5)
 */

import OpenAI from "openai";

const openai = new OpenAI({ apiKey: process.env.OPENAI_API_KEY });

/** ======= PROMPTS REFINADOS ======= */

// Prompt-base (identidade + regras + tom)
const SYSTEM_PROMPT_BASE = `
Voc√™ √© o **M√©dicoHelp**, um assistente cl√≠nico m√©dico destinado EXCLUSIVAMENTE a medicina.
Fale como um m√©dico experiente de pronto-socorro: direto, objetivo, humano, sem floreios.
Valorize o jeito tradicional de registrar: quando o usu√°rio usar CAIXA ALTA, mantenha; quando ele usar abrevia√ß√µes (BEG, LOTE, MV+), respeite.
Nunca troque o termo do m√©dico por outro mais "protocolar". Se ele disser "GRIPE", n√£o mude para "s√≠ndrome gripal". 
Se precisar adicionar precis√£o, fa√ßa ap√≥s o termo do m√©dico, entre par√™nteses (ex.: GRIPE ‚Äî prov√°vel etiologia viral).

**Escopo:** s√≥ responda sobre medicina (cl√≠nica, protocolos, condutas, interpreta√ß√£o de exames, posologia, triagem, encaminhamentos, documenta√ß√£o assistencial).
Se vier tema fora da medicina, responda: "O M√©dicoHelp responde apenas sobre medicina."

**Conduta e seguran√ßa:**
- N√£o invente sinais vitais nem dados do exame f√≠sico. Se forem necess√°rios para conduzir, pe√ßa: "Doutor, me informe PA/FC/FR/Sat/T."
- Sempre destaque sinais de alarme, condutas imediatas e quando reavaliar/encaminhar.
- Use doses pedi√°tricas por kg quando aplic√°vel e m√°ximos por dose/dia (quando forem cr√≠ticos).
- Se houver risco legal/√©tico (ex.: prescri√ß√£o controlada), oriente avalia√ß√£o presencial quando indicado.

**Estilo fixo:**
- Comece com "Beleza, {{NOME_MEDICO}}. Vamos direto ao ponto:" quando houver nome dispon√≠vel.
- Estruture, quando fizer sentido:
  ü©∫ Diagn√≥stico prov√°vel / Impress√£o cl√≠nica  
  ‚ö° Conduta imediata  
  üß™ Investiga√ß√£o complementar  
  üí¨ Observa√ß√µes / Sinais de alarme  
  üìá CID sugerido (quando aplic√°vel)
- Em pedidos de "HIST√ìRIA CL√çNICA", produza em CAIXA ALTA no formato que o m√©dico j√° iniciou, sem enfeitar.
`;

// Prompt do modo CL√çNICO: decis√£o e conduta, com foco em plant√£o
const MODE_CLINICO = `
**MODO: CL√çNICO**
Responda como colega no plant√£o, direto ao ponto. D√™ impress√£o cl√≠nica, conduta com doses, e alertas. 
Se faltar dado essencial para decis√£o imediata, pe√ßa em UMA linha no topo (ex.: "Preciso da PA e SatO2.").

Use o formato estruturado com emojis:
ü©∫ Diagn√≥stico prov√°vel
‚ö° Conduta imediata (com doses e vias)
üß™ Investiga√ß√£o complementar
üí¨ Observa√ß√µes / Sinais de alarme
üìá CID sugerido (quando aplic√°vel)
`;

// Prompt do modo EXPLICATIVO+EVID√äNCIAS: did√°tica + refer√™ncias
const MODE_EXPLICATIVO = `
**MODO: EXPLICATIVO + EVID√äNCIAS**
Explique o racioc√≠nio de forma clara e breve, cite diretrizes/consensos de forma gen√©rica (ex.: "consensos pedi√°tricos, AHA/ACC, IDSA, OMS, SBC, AMB, CFM") sem link externo. 
Quando √∫til, acrescente classes de recomenda√ß√£o/n√≠vel de evid√™ncia de forma sucinta.

Mantenha estrutura did√°tica:
üëâ Conceito / Fisiopatologia (breve)
üìö Evid√™ncias cl√≠nicas / Diretrizes
‚ö° Aplica√ß√£o pr√°tica
üí° Pontos-chave para memorizar
`;

/**
 * Fun√ß√£o auxiliar: verifica se texto √© sobre medicina
 */
function isMedicalContent(text: string): boolean {
  const textoLower = text.toLowerCase();
  return /paciente|dor|febre|press[a√£]o|exame|conduta|diagn[o√≥]stico|cid|posologia|dose|s[a√£]t|sintoma|crise|gestante|trauma|asma|iam|avc|uti|antibi[o√≥]tico|antit[e√©]tico|pronto|ecg|raio.?x|hemograma|gasometria|bpm|mmhg/i.test(
    textoLower
  );
}

/**
 * Extrai primeiro nome do m√©dico
 */
function extractFirstName(fullName: string | undefined): string {
  if (!fullName) return "";
  const clean = fullName.replace(/^Dr\.?\s*/i, "").trim();
  return clean.split(" ")[0] || clean;
}

/**
 * Interface para op√ß√µes do chat
 */
export interface MedicoHelpOptions {
  mode: "clinico" | "explicativo";
  nomeMedico?: string;
  evidenceContext?: string;
}

/**
 * Streaming callback type
 */
export type StreamCallback = (chunk: string) => void;

/**
 * Verifica se GPT-5 est√° dispon√≠vel
 */
async function isGPT5Available(): Promise<boolean> {
  try {
    // Tenta listar modelos dispon√≠veis
    const models = await openai.models.list();
    const modelList = Array.from(models.data || []);
    return modelList.some((m: any) => m.id?.includes("gpt-5") || m.id?.includes("o3"));
  } catch (error) {
    console.log("Could not check GPT-5 availability:", error);
    return false;
  }
}

/**
 * Ask M√©dicoHelp com GPT-5 (ou fallback para GPT-4o)
 * Suporta streaming em tempo real via callback
 * USA NOVA API: client.responses.stream()
 */
export async function askMedicoHelpStreaming(
  userText: string,
  options: MedicoHelpOptions,
  onChunk: StreamCallback
): Promise<{ fullText: string; model: string; tokens: number }> {
  const isMedical = isMedicalContent(userText);
  const firstName = extractFirstName(options.nomeMedico);
  
  // Montar system prompt
  let systemPrompt = SYSTEM_PROMPT_BASE.replace(
    "{{NOME_MEDICO}}",
    firstName || "Doutor"
  );

  // Adicionar modo
  const modePrompt = options.mode === "clinico" ? MODE_CLINICO : MODE_EXPLICATIVO;
  systemPrompt += "\n\n" + modePrompt;

  // Adicionar evid√™ncias se modo explicativo
  if (options.mode === "explicativo" && options.evidenceContext) {
    systemPrompt += `\n\n**Evid√™ncias cient√≠ficas encontradas:**\n${options.evidenceContext}`;
  }

  // Montar mensagens usando novo formato "input"
  const inputMessages: any[] = [
    { role: "system", content: systemPrompt },
  ];

  // Se n√£o for m√©dico, adicionar aviso
  if (!isMedical) {
    inputMessages.push({
      role: "user",
      content: "Lembre-se: responda apenas sobre medicina. Se o texto a seguir n√£o for m√©dico, diga isso em UMA linha."
    });
  } else {
    inputMessages.push({
      role: "user",
      content: "Responda abaixo mantendo o MESMO estilo e termos do usu√°rio (n√£o troque os termos cl√≠nicos que ele usou)."
    });
  }

  inputMessages.push({ role: "user", content: userText });

  // Determinar modelo
  const gpt5Available = await isGPT5Available();
  const modelToUse = gpt5Available ? "gpt-5" : "gpt-4o";
  
  console.log(`ü§ñ Usando modelo: ${modelToUse}${gpt5Available ? " (GPT-5 ativo!)" : " (fallback GPT-4o)"}`);

  let fullText = "";
  let tokenCount = 0;

  try {
    // Tentar streaming com NOVA API
    const stream = await (openai as any).responses.stream({
      model: modelToUse,
      input: inputMessages,
      temperature: 0.4,
      max_output_tokens: modelToUse === "gpt-5" ? 900 : 4096,
    });

    // Processar chunks usando novo formato de eventos
    for await (const event of stream) {
      if (event.type === "response.output_text.delta") {
        const content = event.delta;
        
        if (content) {
          fullText += content;
          tokenCount++;
          onChunk(content);
        }
      }
    }

    console.log(`‚úÖ ${modelToUse} streaming: ${tokenCount} tokens`);
    return { fullText, model: modelToUse, tokens: tokenCount };
    
  } catch (error: any) {
    console.log("‚ö†Ô∏è Erro no streaming GPT-5, tentando fallback:", error.message);
    
    // Fallback 1: Tentar GPT-4o com nova API
    if (modelToUse === "gpt-5") {
      try {
        console.log("üîÑ Fallback para GPT-4o com nova API...");
        const stream = await (openai as any).responses.stream({
          model: "gpt-4o",
          input: inputMessages,
          temperature: 0.4,
          max_output_tokens: 4096,
        });

        fullText = "";
        tokenCount = 0;

        for await (const event of stream) {
          if (event.type === "response.output_text.delta") {
            const content = event.delta;
            
            if (content) {
              fullText += content;
              tokenCount++;
              onChunk(content);
            }
          }
        }

        console.log(`‚úÖ gpt-4o streaming fallback: ${tokenCount} tokens`);
        return { fullText, model: "gpt-4o", tokens: tokenCount };
      } catch (fallbackError: any) {
        console.log("‚ö†Ô∏è Nova API falhou, tentando API legada...", fallbackError.message);
      }
    }

    // Fallback 2: API legada (chat.completions)
    try {
      console.log("üîÑ Usando API legada (chat.completions.create)...");
      const stream = await openai.chat.completions.create({
        model: "gpt-4o",
        messages: inputMessages,
        temperature: 0.4,
        max_tokens: 4096,
        stream: true,
      });

      fullText = "";
      tokenCount = 0;

      for await (const chunk of stream) {
        const content = chunk.choices[0]?.delta?.content;
        
        if (content) {
          fullText += content;
          tokenCount++;
          onChunk(content);
        }
      }

      console.log(`‚úÖ gpt-4o (legado) streaming: ${tokenCount} tokens`);
      return { fullText, model: "gpt-4o", tokens: tokenCount };
    } catch (legacyError: any) {
      console.error("‚ùå Todas as tentativas de streaming falharam:", legacyError);
      throw new Error("Streaming n√£o dispon√≠vel. Tente novamente.");
    }
  }
}

/**
 * Vers√£o n√£o-streaming (fallback)
 * USA NOVA API: client.responses.create()
 */
export async function askMedicoHelpNonStreaming(
  userText: string,
  options: MedicoHelpOptions
): Promise<{ fullText: string; model: string; tokens: number }> {
  const isMedical = isMedicalContent(userText);
  const firstName = extractFirstName(options.nomeMedico);
  
  // Montar system prompt
  let systemPrompt = SYSTEM_PROMPT_BASE.replace(
    "{{NOME_MEDICO}}",
    firstName || "Doutor"
  );

  const modePrompt = options.mode === "clinico" ? MODE_CLINICO : MODE_EXPLICATIVO;
  systemPrompt += "\n\n" + modePrompt;

  if (options.mode === "explicativo" && options.evidenceContext) {
    systemPrompt += `\n\n**Evid√™ncias cient√≠ficas encontradas:**\n${options.evidenceContext}`;
  }

  const inputMessages: any[] = [
    { role: "system", content: systemPrompt },
  ];

  if (!isMedical) {
    inputMessages.push({
      role: "user",
      content: "Lembre-se: responda apenas sobre medicina. Se o texto a seguir n√£o for m√©dico, diga isso em UMA linha."
    });
  } else {
    inputMessages.push({
      role: "user",
      content: "Responda abaixo mantendo o MESMO estilo e termos do usu√°rio (n√£o troque os termos cl√≠nicos que ele usou)."
    });
  }

  inputMessages.push({ role: "user", content: userText });

  const gpt5Available = await isGPT5Available();
  const modelToUse = gpt5Available ? "gpt-5" : "gpt-4o";
  
  console.log(`ü§ñ Usando modelo: ${modelToUse} (non-streaming)${gpt5Available ? " (GPT-5 ativo!)" : " (fallback GPT-4o)"}`);

  try {
    // Tentar NOVA API
    const response = await (openai as any).responses.create({
      model: modelToUse,
      input: inputMessages,
      temperature: 0.4,
      max_output_tokens: modelToUse === "gpt-5" ? 900 : 4096,
    });

    const fullText = response.output_text || "Desculpe, n√£o foi poss√≠vel processar sua pergunta.";
    const tokens = fullText.length;

    console.log(`‚úÖ ${modelToUse} non-streaming: ${tokens} chars`);
    return { fullText, model: modelToUse, tokens };
    
  } catch (error: any) {
    console.log("‚ö†Ô∏è Nova API falhou, tentando API legada...", error.message);
    
    // Fallback para API legada
    try {
      const completion = await openai.chat.completions.create({
        model: "gpt-4o",
        messages: inputMessages,
        temperature: 0.4,
        max_tokens: 4096,
        stream: false,
      });

      const fullText = completion.choices[0]?.message?.content || "Desculpe, n√£o foi poss√≠vel processar sua pergunta.";
      const tokens = fullText.length;

      console.log(`‚úÖ gpt-4o (legado) non-streaming: ${tokens} chars`);
      return { fullText, model: "gpt-4o", tokens };
    } catch (legacyError: any) {
      console.error("‚ùå Todas as tentativas falharam:", legacyError);
      throw new Error("N√£o foi poss√≠vel processar sua pergunta. Tente novamente.");
    }
  }
}
