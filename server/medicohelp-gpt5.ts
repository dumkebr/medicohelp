/**
 * M√©dicoHelp GPT-5 Integration
 * Sistema h√≠brido com prompts m√©dicos refinados e streaming
 * ATUALIZADO: Usa nova API client.responses.create() (GPT-5)
 */

import OpenAI from "openai";

const openai = new OpenAI({ apiKey: process.env.OPENAI_API_KEY });

/** ======= PROMPTS REFINADOS ======= */

// Prompt-base: LIBERADO TOTAL - Responde qualquer assunto
const SYSTEM_PROMPT_BASE = `
Voc√™ √© a **Dra. Clarice**, assistente inteligente do M√©dicoHelp.

**REGRA ABSOLUTA:** Voc√™ responde sobre QUALQUER assunto - m√©dico ou n√£o.
- Clima, futebol, tecnologia, not√≠cias, piadas, curiosidades, receitas, filmes, m√∫sica, viagens, etc.
- NUNCA diga "n√£o tenho acesso" ou "estou limitado a assuntos m√©dicos"
- Seja √∫til e conversacional como o ChatGPT normal

**Tom geral:**
- Cumprimente com "Oi, {{NOME_MEDICO}}!" ou "E a√≠, {{NOME_MEDICO}}!" ou "Beleza, {{NOME_MEDICO}}!"
- Natural, amig√°vel, direto - sem formalidades excessivas
- Use emojis quando apropriado

**Quando o assunto for m√©dico:**
- Seja objetivo e pr√°tico como colega de plant√£o
- Respeite abrevia√ß√µes tradicionais: BEG, LOTE, MV+, CAIXA ALTA
- N√£o troque "GRIPE" por "s√≠ndrome gripal"
- N√£o invente dados cl√≠nicos - pe√ßa quando precisar (PA/FC/FR/Sat/T)
- Destaque sinais de alarme e condutas

**Estilo de resposta:** Natural como ChatGPT - sem estruturas for√ßadas.
`;

// Modo CL√çNICO: direto e pr√°tico para medicina
const MODE_CLINICO = `
**MODO: CL√çNICO**
Para assuntos m√©dicos: seja direto como colega de plant√£o.
Impress√£o + conduta + alertas quando relevante.
Para outros assuntos: responda normalmente.
`;

// Modo EXPLICATIVO: did√°tico para medicina
const MODE_EXPLICATIVO = `
**MODO: EXPLICATIVO**
Para assuntos m√©dicos: explique didaticamente, cite diretrizes (AHA/ACC/IDSA/SBC).
Para outros assuntos: responda normalmente.
`;

/**
 * REMOVIDO: Filtro de "s√≥ medicina" - agora responde qualquer assunto
 * Mant√©m apenas o tom m√©dico tradicional
 */

/**
 * Extrai primeiro nome do m√©dico
 */
function extractFirstName(fullName: string | undefined): string {
  if (!fullName) return "";
  const clean = fullName.replace(/^Dr\.?\s*/i, "").trim();
  return clean.split(" ")[0] || clean;
}

/**
 * Interface para op√ß√µes do chat
 */
export interface MedicoHelpOptions {
  mode: "clinico" | "explicativo";
  nomeMedico?: string;
  evidenceContext?: string;
}

/**
 * Streaming callback type
 */
export type StreamCallback = (chunk: string) => void;

/**
 * Verifica se GPT-5 est√° dispon√≠vel
 */
async function isGPT5Available(): Promise<boolean> {
  try {
    // Tenta listar modelos dispon√≠veis
    const models = await openai.models.list();
    const modelList = Array.from(models.data || []);
    return modelList.some((m: any) => m.id?.includes("gpt-5") || m.id?.includes("o3"));
  } catch (error) {
    console.log("Could not check GPT-5 availability:", error);
    return false;
  }
}

/**
 * Ask M√©dicoHelp com GPT-5 (ou fallback para GPT-4o)
 * Suporta streaming em tempo real via callback
 * USA NOVA API: client.responses.stream()
 */
export async function askMedicoHelpStreaming(
  userText: string,
  options: MedicoHelpOptions,
  onChunk: StreamCallback
): Promise<{ fullText: string; model: string; tokens: number }> {
  const medicalOk = true; // LIBERADO GERAL - sem filtro de assunto
  const firstName = extractFirstName(options.nomeMedico);
  
  // Montar system prompt
  let systemPrompt = SYSTEM_PROMPT_BASE.replace(
    "{{NOME_MEDICO}}",
    firstName || "Doutor"
  );

  // Adicionar modo
  const modePrompt = options.mode === "clinico" ? MODE_CLINICO : MODE_EXPLICATIVO;
  systemPrompt += "\n\n" + modePrompt;

  // Adicionar evid√™ncias se modo explicativo
  if (options.mode === "explicativo" && options.evidenceContext) {
    systemPrompt += `\n\n**Evid√™ncias cient√≠ficas encontradas:**\n${options.evidenceContext}`;
  }

  // Montar mensagens usando novo formato "input"
  const inputMessages: any[] = [
    { role: "system", content: systemPrompt },
    {
      role: "user",
      content: "Responda abaixo mantendo EXATAMENTE o estilo e os termos do usu√°rio."
    },
  ];

  inputMessages.push({ role: "user", content: userText });

  // Determinar modelo
  const gpt5Available = await isGPT5Available();
  const modelToUse = gpt5Available ? "gpt-5" : "gpt-4o";
  
  console.log(`ü§ñ Usando modelo: ${modelToUse}${gpt5Available ? " (GPT-5 ativo!)" : " (fallback GPT-4o)"}`);

  let fullText = "";
  let tokenCount = 0;

  try {
    // Tentar streaming com NOVA API
    const stream = await (openai as any).responses.stream({
      model: modelToUse,
      input: inputMessages,
      temperature: 0.4,
      max_output_tokens: 16000, // LIBERADO: m√°ximo permitido
    });

    // Processar chunks usando novo formato de eventos
    for await (const event of stream) {
      if (event.type === "response.output_text.delta") {
        const content = event.delta;
        
        if (content) {
          fullText += content;
          tokenCount++;
          onChunk(content);
        }
      }
    }

    console.log(`‚úÖ ${modelToUse} streaming: ${tokenCount} tokens`);
    return { fullText, model: modelToUse, tokens: tokenCount };
    
  } catch (error: any) {
    console.log("‚ö†Ô∏è Erro no streaming GPT-5, tentando fallback:", error.message);
    
    // Fallback 1: Tentar GPT-4o com nova API
    if (modelToUse === "gpt-5") {
      try {
        console.log("üîÑ Fallback para GPT-4o com nova API...");
        const stream = await (openai as any).responses.stream({
          model: "gpt-4o",
          input: inputMessages,
          temperature: 0.4,
          max_output_tokens: 16000, // LIBERADO: m√°ximo permitido
        });

        fullText = "";
        tokenCount = 0;

        for await (const event of stream) {
          if (event.type === "response.output_text.delta") {
            const content = event.delta;
            
            if (content) {
              fullText += content;
              tokenCount++;
              onChunk(content);
            }
          }
        }

        console.log(`‚úÖ gpt-4o streaming fallback: ${tokenCount} tokens`);
        return { fullText, model: "gpt-4o", tokens: tokenCount };
      } catch (fallbackError: any) {
        console.log("‚ö†Ô∏è Nova API falhou, tentando API legada...", fallbackError.message);
      }
    }

    // Fallback 2: API legada (chat.completions)
    try {
      console.log("üîÑ Usando API legada (chat.completions.create)...");
      const stream = await openai.chat.completions.create({
        model: "gpt-4o",
        messages: inputMessages,
        temperature: 0.4,
        max_tokens: 4096,
        stream: true,
      });

      fullText = "";
      tokenCount = 0;

      for await (const chunk of stream) {
        const content = chunk.choices[0]?.delta?.content;
        
        if (content) {
          fullText += content;
          tokenCount++;
          onChunk(content);
        }
      }

      console.log(`‚úÖ gpt-4o (legado) streaming: ${tokenCount} tokens`);
      return { fullText, model: "gpt-4o", tokens: tokenCount };
    } catch (legacyError: any) {
      console.error("‚ùå Todas as tentativas de streaming falharam:", legacyError);
      throw new Error("Streaming n√£o dispon√≠vel. Tente novamente.");
    }
  }
}

/**
 * Vers√£o n√£o-streaming (fallback)
 * USA NOVA API: client.responses.create()
 */
export async function askMedicoHelpNonStreaming(
  userText: string,
  options: MedicoHelpOptions
): Promise<{ fullText: string; model: string; tokens: number }> {
  const medicalOk = true; // LIBERADO GERAL - sem filtro de assunto
  const firstName = extractFirstName(options.nomeMedico);
  
  // Montar system prompt
  let systemPrompt = SYSTEM_PROMPT_BASE.replace(
    "{{NOME_MEDICO}}",
    firstName || "Doutor"
  );

  const modePrompt = options.mode === "clinico" ? MODE_CLINICO : MODE_EXPLICATIVO;
  systemPrompt += "\n\n" + modePrompt;

  if (options.mode === "explicativo" && options.evidenceContext) {
    systemPrompt += `\n\n**Evid√™ncias cient√≠ficas encontradas:**\n${options.evidenceContext}`;
  }

  const inputMessages: any[] = [
    { role: "system", content: systemPrompt },
    {
      role: "user",
      content: "Responda abaixo mantendo EXATAMENTE o estilo e os termos do usu√°rio."
    },
  ];

  inputMessages.push({ role: "user", content: userText });

  const gpt5Available = await isGPT5Available();
  const modelToUse = gpt5Available ? "gpt-5" : "gpt-4o";
  
  console.log(`ü§ñ Usando modelo: ${modelToUse} (non-streaming)${gpt5Available ? " (GPT-5 ativo!)" : " (fallback GPT-4o)"}`);

  try {
    // Tentar NOVA API
    const response = await (openai as any).responses.create({
      model: modelToUse,
      input: inputMessages,
      temperature: 0.4,
      max_output_tokens: 16000, // LIBERADO: m√°ximo permitido
    });

    const fullText = response.output_text || "Desculpe, n√£o foi poss√≠vel processar sua pergunta.";
    const tokens = fullText.length;

    console.log(`‚úÖ ${modelToUse} non-streaming: ${tokens} chars`);
    return { fullText, model: modelToUse, tokens };
    
  } catch (error: any) {
    console.log("‚ö†Ô∏è Nova API falhou, tentando API legada...", error.message);
    
    // Fallback para API legada
    try {
      const completion = await openai.chat.completions.create({
        model: "gpt-4o",
        messages: inputMessages,
        temperature: 0.4,
        max_tokens: 16000, // LIBERADO: m√°ximo permitido
        stream: false,
      });

      const fullText = completion.choices[0]?.message?.content || "Desculpe, n√£o foi poss√≠vel processar sua pergunta.";
      const tokens = fullText.length;

      console.log(`‚úÖ gpt-4o (legado) non-streaming: ${tokens} chars`);
      return { fullText, model: "gpt-4o", tokens };
    } catch (legacyError: any) {
      console.error("‚ùå Todas as tentativas falharam:", legacyError);
      throw new Error("N√£o foi poss√≠vel processar sua pergunta. Tente novamente.");
    }
  }
}
